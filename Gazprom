# ============================================================
# УСТАНОВКА БИБЛИОТЕК
# ============================================================
# !pip install xgboost scikit-learn pandas numpy

# ============================================================
# ИМПОРТЫ
# ============================================================
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from xgboost import XGBRegressor

# ============================================================
# КОНФИГ
# ============================================================
CSV_PATH = "data.csv"
DATE_START = "Дата начала поиска"
DATE_END   = "Дата окончания поиска"
TARGET_COL = "Срок поиска артефакта"  # часы

RANDOM_STATE = 42
TEST_SIZE = 0.2

# Сетка гиперпараметров (умеренная по размеру; расширяйте при желании)
PARAM_GRID = {
    "model__n_estimators": [200, 400, 800],
    "model__learning_rate": [0.03, 0.05, 0.1],
    "model__max_depth": [4, 6, 8],
    "model__subsample": [0.7, 0.9, 1.0],
    "model__colsample_bytree": [0.7, 0.9, 1.0],
    "model__reg_lambda": [1.0, 2.0, 5.0],
}

# ============================================================
# ЗАГРУЗКА И ПОДГОТОВКА ДАННЫХ
# ============================================================
df = pd.read_csv(CSV_PATH)

# Оставляем только строки, где есть дата окончания
df = df[pd.notna(df[DATE_END])].copy()

# Преобразуем в datetime
df[DATE_START] = pd.to_datetime(df[DATE_START], errors="coerce")
df[DATE_END]   = pd.to_datetime(df[DATE_END], errors="coerce")

# Считаем целевую: длительность в часах
df[TARGET_COL] = (df[DATE_END] - df[DATE_START]).dt.total_seconds() / 3600.0

# Удалим строки с некорректными/отрицательными/пустыми целевыми
df = df[(df[TARGET_COL].notna()) & (df[TARGET_COL] >= 0)].copy()

# Фичи = все, кроме исходных дат и таргета
drop_cols = [DATE_START, DATE_END, TARGET_COL]
feature_cols = [c for c in df.columns if c not in drop_cols]
X = df[feature_cols].copy()
y = df[TARGET_COL].values

# Обработка бесконечностей/NaN
X = X.replace([np.inf, -np.inf], np.nan)

# Разделение
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE
)

# ============================================================
# ПРЕДОБРАБОТКА: OneHot для object, passthrough для чисел
# (XGBRegressor не требует стандартизации)
# ============================================================
cat_cols = X_train.select_dtypes(include=["object", "category"]).columns.tolist()
num_cols = [c for c in X_train.columns if c not in cat_cols]

preprocess = ColumnTransformer(
    transformers=[
        ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), cat_cols),
        ("num", "passthrough", num_cols),
    ],
    remainder="drop",
)

# ============================================================
# ЛУЧШАЯ МОДЕЛЬ: XGBRegressor (+ GridSearchCV)
# ============================================================
xgb = XGBRegressor(
    objective="reg:squarederror",
    random_state=RANDOM_STATE,
    n_jobs=-1,
    tree_method="hist"  # обычно быстрее
)

pipe = Pipeline(steps=[
    ("prep", preprocess),
    ("model", xgb),
])

grid = GridSearchCV(
    estimator=pipe,
    param_grid=PARAM_GRID,
    scoring="neg_root_mean_squared_error",
    cv=5,
    n_jobs=-1,
    verbose=1,
)

grid.fit(X_train, y_train)

best_model = grid.best_estimator_
print("Лучшие параметры:", grid.best_params_)

# ============================================================
# ОЦЕНКА
# ============================================================
y_pred = best_model.predict(X_test)
rmse = mean_squared_error(y_test, y_pred, squared=False)
mae  = mean_absolute_error(y_test, y_pred)
r2   = r2_score(y_test, y_pred)

print(f"RMSE: {rmse:.4f}")
print(f"MAE : {mae:.4f}")
print(f"R^2 : {r2:.4f}")

# ============================================================
# ВАЖНОСТИ ФИЧЕЙ (с учетом OHE)
# ============================================================
# Достаём имена фич после трансформации
ohe = best_model.named_steps["prep"].named_transformers_["cat"]
ohe_feature_names = []
if len(cat_cols) > 0:
    ohe_feature_names = ohe.get_feature_names_out(cat_cols).tolist()

final_feature_names = ohe_feature_names + num_cols

# Вытаскиваем важности из модели
xgb_in_pipe = best_model.named_steps["model"]
importances = xgb_in_pipe.feature_importances_

feat_imp = (
    pd.DataFrame({"feature": final_feature_names, "importance": importances})
    .sort_values("importance", ascending=False)
    .reset_index(drop=True)
)

print("\nТоп-20 признаков по важности:")
print(feat_imp.head(20))
